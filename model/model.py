# -*- coding: utf-8 -*-
"""MODELph.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CtVb8h__0aZuwgGFJDbrHPyvsqXzq_GO

#Software para reconocimiento de notas musicales para estudio de voz con tensoflow
"""

from google.colab import drive
drive.mount('/content/drive')

# importación de librerias
import pandas as pd
import numpy as np
import os
import tensorflow as tf
import librosa
from sklearn.model_selection import train_test_split
from tensorflow.python.keras.layers import Dense

# Preprocesado y modelado
# ==============================================================================
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_squared_error


# Configuración warnings
# ==============================================================================
import warnings
warnings.filterwarnings('ignore')

# Ruta al directorio que contiene los archivos de audio en formato .wav
data_dir = '/content/drive/MyDrive/Trumpet_C'

# Obtener la lista de archivos de audio
file_list = os.listdir(data_dir)

# Variables para almacenar las muestras y etiquetas
samples = []
labels = []

# Definir la longitud máxima deseada para las muestras
max_length = 100

# Procesar cada archivo de audio
for file in file_list:
    # Cargar el archivo de audio

    audio_data, sr = librosa.load("/content/drive/MyDrive/Trumpet_C/TpC-ord-A#3-ff-N-T23u_R100u.wav", sr=None)

    # Truncar o rellenar la muestra para que tenga la longitud máxima deseada
    if len(audio_data) > max_length:
        processed_audio_data = audio_data[:max_length]
    else:
        processed_audio_data = np.pad(audio_data, (0, max_length - len(audio_data)), mode='constant')

    # Definir y asignar el valor de la etiqueta
    label = 1

    # Agregar la muestra procesada y la etiqueta a las listas
    samples.append(processed_audio_data)
    labels.append(label)

# Convertir las listas en arrays numpy
samples = np.array(samples)
labels = np.array(labels)

# Obtener el número de características
n_features = samples.shape[1]

# Dividir los datos en conjuntos de entrenamiento y validación
train_samples, val_samples, train_labels, val_labels = train_test_split(samples, labels, test_size=0.2, random_state=42)

num_classes = 1

# Definir el modelo
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(n_features,)),
    # Agregar capas adicionales según sea necesario
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

# Compilar el modelo
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Entrenar el modelo
model.fit(train_samples, train_labels, validation_data=(val_samples, val_labels), epochs=20)

# Guardar el modelo entrenado
model.save('modelo.h5')